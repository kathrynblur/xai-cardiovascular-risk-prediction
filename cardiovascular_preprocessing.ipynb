{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8265611-3b8c-4555-95ab-16ce14daee61",
   "metadata": {},
   "source": [
    "# Cardiovascular Disease Risk Prediction: Data Preprocessing Pipeline\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook implements a **rigorous, clinical-grade data preprocessing pipeline** for cardiovascular disease risk stratification. The workflow transforms raw clinical data into **model-ready artifacts** suitable for reproducible machine learning research and HPC deployment.\n",
    "\n",
    "### Core Principles\n",
    "\n",
    "1. **No Data Leakage**: Transformers are fit on training data only; test set remains truly \"unseen\"\n",
    "2. **Clinical Validity**: Stratified splitting preserves population prevalence (~8% disease rate)\n",
    "3. **Type-Specific Preprocessing**: Different feature types receive appropriate transformations\n",
    "4. **HPC Optimization**: Dense NumPy arrays enable GPU acceleration (XGBoost CUDA, CatBoost GPU)\n",
    "5. **Reproducibility**: Fixed random states and explicit configuration ensure deterministic runs\n",
    "\n",
    "### Clinical Context\n",
    "\n",
    "Cardiovascular disease screening is a **high-stakes application** where:\n",
    "- **False Negatives (FN)**: Missing an at-risk patient → disease progression, cardiac event, mortality\n",
    "- **False Positives (FP)**: Unnecessary follow-up → anxiety, cost, but no clinical risk\n",
    "\n",
    "The cost asymmetry is **severe**. We prioritize recall (minimize FN) and accept more FP as the clinical trade-off.\n",
    "\n",
    "### Workflow Overview\n",
    "\n",
    "| Phase | Input | Output | Purpose |\n",
    "|-------|-------|--------|---------|  \n",
    "| **1. Load & Split** | `HeartDisease.csv` | Train/Test (stratified 80/20) | Ensure consistent class ratios |\n",
    "| **2. Analyze Imbalance** | Train/Test labels | Class distribution report | Verify stratification; confirm ~8% positive rate |\n",
    "| **3. Feature Engineering** | Raw feature columns | Categorized feature groups | Prepare for type-specific transformations |\n",
    "| **4. Build Pipeline** | Feature specifications | `ColumnTransformer` object | Encapsulate preprocessing logic |\n",
    "| **5. Fit & Transform** | Training data | Preprocessed arrays | Learn parameters from train only |\n",
    "| **6. Validate** | Transformed data | Sanity check report | Confirm zero missing values, consistent shapes |\n",
    "| **7. Export** | Preprocessed arrays | Compressed `.joblib` files | HPC-ready artifacts for modeling |\n",
    "\n",
    "### Handoff Artifacts\n",
    "\n",
    "| File | Format | Size | Used By |\n",
    "|------|--------|------|---------|  \n",
    "| `X_train_ready.joblib` | Dense NumPy array | ~2–5 MB | Model training on GPU |\n",
    "| `X_test_ready.joblib` | Dense NumPy array | ~0.5–1 MB | Model evaluation |\n",
    "| `y_train_ready.joblib` | 1D array (0/1) | <1 MB | Classification labels |\n",
    "| `y_test_ready.joblib` | 1D array (0/1) | <1 MB | Evaluation labels |\n",
    "| `feature_names.joblib` | List of strings | <1 MB | SHAP/LIME interpretability |\n",
    "| `preprocessor.joblib` | Fitted transformer | ~1 MB | Production inference pipelines |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f45282-c1f7-4111-8426-749d6cc2b847",
   "metadata": {},
   "source": [
    "## Step 0: Environment Configuration & Reproducibility Setup\n",
    "\n",
    "### Software Dependencies\n",
    "\n",
    "This preprocessing pipeline uses **only essential libraries** to minimize dependencies and maximize reproducibility:\n",
    "\n",
    "| Library | Version | Role |\n",
    "|---------|---------|------|  \n",
    "| `pandas` | ≥1.3.0 | DataFrame manipulation, type handling |\n",
    "| `numpy` | ≥1.21.0 | Numerical computing, array operations |\n",
    "| `scikit-learn` | ≥1.0.0 | Preprocessing pipelines, transformers |\n",
    "| `joblib` | ≥1.1.0 | Serialization of transformers and models |\n",
    "| `pathlib` | Standard library | Cross-platform file path operations |\n",
    "\n",
    "**Intentionally Excluded**: XGBoost, CatBoost, Optuna (reserved for modeling phase)\n",
    "\n",
    "### Reproducibility & Determinism\n",
    "\n",
    "To ensure **bit-for-bit reproducibility** across runs and platforms:\n",
    "\n",
    "```python\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "```\n",
    "\n",
    "This seed is propagated through stratified train-test splitting and any future cross-validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e37a12-a1f6-456e-8dd9-574acc917540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREPROCESSING NOTEBOOK INITIALIZED\n",
      "============================================================\n",
      "Output directory: C:\\Users\\natha\\code\\github\\xai-cardiovascular-risk-prediction\\processed\n",
      "Random state: 42\n"
     ]
    }
   ],
   "source": [
    "# Core libraries for preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Scikit-learn preprocessing components\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Paths\n",
    "PROCESSED_DIR = Path(\"processed\")\n",
    "PROCESSED_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPROCESSING NOTEBOOK INITIALIZED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Output directory: {PROCESSED_DIR.resolve()}\")\n",
    "print(f\"Random state: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32df07a-cb35-49bc-956e-6dd36d4fb239",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading & Stratified Train-Test Splitting\n",
    "\n",
    "### Why Stratification Matters\n",
    "\n",
    "The cardiovascular dataset exhibits **severe class imbalance**: approximately **8% of patients have diagnosed heart disease**. Without stratification, random splitting can create inconsistent class ratios between train and test sets, leading to:\n",
    "\n",
    "- Inconsistent class distributions between sets\n",
    "- Biased performance estimates  \n",
    "- High variance across cross-validation folds\n",
    "\n",
    "**Stratified splitting** ensures both train and test sets maintain the original class ratio:\n",
    "\n",
    "$$\\frac{n_+^{\\text{train}}}{n^{\\text{train}}} \\approx \\frac{n_+^{\\text{test}}}{n^{\\text{test}}} \\approx \\frac{n_+^{\\text{total}}}{n^{\\text{total}}} = p \\approx 0.08$$\n",
    "\n",
    "### Dual-Mode Loading Strategy\n",
    "\n",
    "The code supports two input scenarios for flexibility:\n",
    "\n",
    "1. **Pre-split Mode** (Preferred): Loads from `processed/X_train_raw.csv`, etc. when available\n",
    "2. **Fallback Mode**: Loads from `HeartDisease.csv` and performs stratified split if intermediate files are missing\n",
    "\n",
    "This design maximizes robustness and maintains reproducibility via `RANDOM_STATE=42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12ab5dbf-b4de-4d93-bb5e-e3e0bf9f6a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created and saved stratified train/test splits to /processed\n",
      "Created Age_num from Age_Category\n",
      "============================================================\n",
      "DATA LOADED SUCCESSFULLY\n",
      "============================================================\n",
      "X_train shape: (247083, 19)\n",
      "X_test shape:  (61771, 19)\n",
      "y_train shape: (247083,)\n",
      "y_test shape:  (61771,)\n",
      "\n",
      "Target dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define expected split files\n",
    "x_train_path = PROCESSED_DIR / \"X_train_raw.csv\"\n",
    "x_test_path = PROCESSED_DIR / \"X_test_raw.csv\"\n",
    "y_train_path = PROCESSED_DIR / \"y_train_raw.csv\"\n",
    "y_test_path = PROCESSED_DIR / \"y_test_raw.csv\"\n",
    "\n",
    "if all(p.exists() for p in [x_train_path, x_test_path, y_train_path, y_test_path]):\n",
    "    # Load pre-split data from EDA\n",
    "    X_train = pd.read_csv(x_train_path)\n",
    "    X_test = pd.read_csv(x_test_path)\n",
    "    y_train = pd.read_csv(y_train_path).squeeze(\"columns\")\n",
    "    y_test = pd.read_csv(y_test_path).squeeze(\"columns\")\n",
    "    print(\"Loaded existing stratified train/test splits from /processed\")\n",
    "else:\n",
    "    # Fallback: create stratified split from the cleaned full dataset\n",
    "    full_data_path = Path(\"HeartDisease.csv\")\n",
    "    if not full_data_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not find pre-split data in /processed or HeartDisease.csv in the project root.\"\n",
    "        )\n",
    "\n",
    "    df = pd.read_csv(full_data_path)\n",
    "    y = df[\"Heart_Disease\"]\n",
    "    X = df.drop(columns=[\"Heart_Disease\"])\n",
    "\n",
    "    # Convert labels to numeric if needed\n",
    "    if y.dtype == \"object\":\n",
    "        y = y.map({\"No\": 0, \"Yes\": 1}).astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "    # Save splits for reproducibility\n",
    "    X_train.to_csv(x_train_path, index=False)\n",
    "    X_test.to_csv(x_test_path, index=False)\n",
    "    y_train.to_csv(y_train_path, index=False)\n",
    "    y_test.to_csv(y_test_path, index=False)\n",
    "    print(\"Created and saved stratified train/test splits to /processed\")\n",
    "\n",
    "# Ensure binary targets are numeric (0/1)\n",
    "if y_train.dtype == \"object\":\n",
    "    y_train = y_train.map({\"No\": 0, \"Yes\": 1}).astype(int)\n",
    "    y_test = y_test.map({\"No\": 0, \"Yes\": 1}).astype(int)\n",
    "\n",
    "# Create Age_num if missing\n",
    "def _age_category_to_num(value: str) -> float:\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    text = str(value).strip()\n",
    "    if text.lower() in {\"80 or older\", \"80+\", \"80+ years\"}:\n",
    "        return 80.0\n",
    "    if \"-\" in text:\n",
    "        parts = text.split(\"-\")\n",
    "        try:\n",
    "            low, high = float(parts[0]), float(parts[1])\n",
    "            return (low + high) / 2\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "if \"Age_num\" not in X_train.columns and \"Age_Category\" in X_train.columns:\n",
    "    X_train[\"Age_num\"] = X_train[\"Age_Category\"].apply(_age_category_to_num)\n",
    "    X_test[\"Age_num\"] = X_test[\"Age_Category\"].apply(_age_category_to_num)\n",
    "    print(\"Created Age_num from Age_Category\")\n",
    "\n",
    "# Display shapes for verification\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA LOADED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape:  {y_test.shape}\")\n",
    "print(f\"\\nTarget dtype: {y_train.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfdeccf-28f4-4afe-b2b5-c27c6352cb33",
   "metadata": {},
   "source": [
    "### Data Preview\n",
    "\n",
    "Quick sanity check to verify data loaded correctly before proceeding with transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe16d059-a360-4a93-9de4-453ecd97666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview training features and labels\n",
    "print(\"Training Features (first 5 rows):\")\n",
    "display(X_train.head())\n",
    "\n",
    "print(\"\\nTraining Labels (first 5 values):\")\n",
    "print(y_train.head())\n",
    "print(f\"\\nLabel distribution: {dict(y_train.value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975d43d7-2b70-48a0-b15a-18f82a144ce4",
   "metadata": {},
   "source": [
    "## Step 2: Class Imbalance Analysis\n",
    "\n",
    "### Clinical Significance\n",
    "\n",
    "**Cardiovascular disease screening is asymmetric**: the cost of different error types is vastly different.\n",
    "\n",
    "| Event | Clinical Consequence | Model Trade-off | Policy |\n",
    "|-------|---------------------|-----------------|--------| \n",
    "| **False Negative (FN)** | Missed patient → disease progression, MI, stroke, death | ❌ Unacceptable | Minimize via high recall (target 85%+) |\n",
    "| **False Positive (FP)** | Unnecessary follow-up → anxiety, cost, no mortality risk | ✓ Acceptable trade-off | Accepted to reduce FN |\n",
    "\n",
    "With ~8% disease prevalence, naive accuracy (predict \"No disease\" for all) = **92%** but sensitivity (recall) = **0%** → clinically worthless.\n",
    "\n",
    "### Our Strategy: Class Weighting\n",
    "\n",
    "We use `class_weight='balanced'` in models rather than synthetic oversampling (SMOTE) because:\n",
    "\n",
    "- **Preserves true population distribution** for calibration\n",
    "- **No synthetic data** – all training examples are real patients  \n",
    "- **Probability estimates remain calibrated** to true prevalence\n",
    "\n",
    "SMOTE distorts prevalence (8% → 50%), making probability estimates meaningless for clinical decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "273af673-fd34-4f60-8207-21c8a8d79d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLASS DISTRIBUTION ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Training Set:\n",
      "  Class 0 (No Disease): 227,106 (91.91%)\n",
      "  Class 1 (Disease):    19,977 (8.09%)\n",
      "\n",
      "Test Set:\n",
      "  Class 0 (No Disease): 56,777 (91.92%)\n",
      "  Class 1 (Disease):    4,994 (8.08%)\n",
      "\n",
      "Stratification check: Train=8.09%, Test=8.08%\n",
      "   → Class ratios are consistent (good stratification)\n"
     ]
    }
   ],
   "source": [
    "def display_class_distribution(y, set_name):\n",
    "    \"\"\"Display class counts and percentages for a target vector.\"\"\"\n",
    "    total = len(y)\n",
    "    positive_count = y.sum()\n",
    "    negative_count = total - positive_count\n",
    "    positive_pct = 100 * positive_count / total\n",
    "    negative_pct = 100 * negative_count / total\n",
    "    \n",
    "    print(f\"\\n{set_name}:\")\n",
    "    print(f\"  Class 0 (No Disease): {negative_count:,} ({negative_pct:.2f}%)\")\n",
    "    print(f\"  Class 1 (Disease):    {positive_count:,} ({positive_pct:.2f}%)\")\n",
    "    return positive_pct\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "train_pct = display_class_distribution(y_train, \"Training Set\")\n",
    "test_pct = display_class_distribution(y_test, \"Test Set\")\n",
    "\n",
    "# Verify stratification worked\n",
    "print(f\"\\nStratification check: Train={train_pct:.2f}%, Test={test_pct:.2f}%\")\n",
    "if abs(train_pct - test_pct) < 1.0:\n",
    "    print(\"   → Class ratios are consistent (good stratification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2cdad7-5432-4572-beeb-845f6be970c4",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering & Type Categorization\n",
    "\n",
    "### Rationale for Feature Type Grouping\n",
    "\n",
    "Different feature types have **inherently different distributions and meanings**. A \"one-size-fits-all\" preprocessing approach fails because:\n",
    "\n",
    "1. **Numeric vs. Categorical**: Numeric features (BMI, height) have continuous ranges; categorical features (health status) have discrete categories\n",
    "2. **Scaling Sensitivity**: Numeric features need scaling for convergence; categorical features need encoding for model compatibility\n",
    "3. **Missing Value Patterns**: Numeric missing values are best handled by median; categorical missing values by mode\n",
    "\n",
    "### Feature Groups\n",
    "\n",
    "#### Numeric Features: Robust Imputation & Standardization\n",
    "\n",
    "**Columns**: Height, Weight, BMI, Alcohol Consumption, Age (numeric), Fruit/Vegetable Consumption, Fried Potato Consumption\n",
    "\n",
    "**Pipeline**: Median Imputation → StandardScaler\n",
    "\n",
    "**Why Median?** Robust to outliers common in medical data (extreme BMI, unusual weight)\n",
    "\n",
    "**Why StandardScaler?** Z-score transformation ($z_i = \\frac{x_i - \\mu}{\\sigma}$) ensures:\n",
    "- Faster gradient descent convergence\n",
    "- Fair regularization penalties across features\n",
    "- GPU numerical stability\n",
    "\n",
    "#### Categorical Features: Mode Imputation & One-Hot Encoding\n",
    "\n",
    "**Columns**: General_Health, Checkup, Diabetes\n",
    "\n",
    "**Pipeline**: Mode Imputation → OneHotEncoder\n",
    "\n",
    "**Why One-Hot?** \n",
    "- Prevents false ordinal relationships\n",
    "- Works with all model types\n",
    "- Explicitly represents category presence/absence\n",
    "\n",
    "**Implementation**: `OneHotEncoder(handle_unknown='ignore', sparse_output=False)`\n",
    "- `handle_unknown='ignore'`: Gracefully handles rare categories in test set\n",
    "- `sparse_output=False`: Dense arrays for GPU compatibility\n",
    "\n",
    "#### Binary Features: Simple Imputation\n",
    "\n",
    "**Columns**: Exercise, Sex, Smoking History, Skin Cancer, Depression, Arthritis, Other Cancer\n",
    "\n",
    "**Pipeline**: Mode Imputation only (already in [0, 1] range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5beaa1b6-0413-4f6d-b4af-52f242b307e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE GROUP DEFINITIONS\n",
      "============================================================\n",
      "Categorical columns: 3\n",
      "Numeric columns:     8\n",
      "Binary columns:      7\n",
      "Columns to drop:     ['Age_Category']\n"
     ]
    }
   ],
   "source": [
    "# Define feature groups explicitly\n",
    "CATEGORICAL_COLS = [\n",
    "    \"General_Health\",\n",
    "    \"Checkup\",\n",
    "    \"Diabetes\",\n",
    "]\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    \"Height_(cm)\",\n",
    "    \"Weight_(kg)\",\n",
    "    \"BMI\",\n",
    "    \"Alcohol_Consumption\",\n",
    "    \"Fruit_Consumption\",\n",
    "    \"Green_Vegetables_Consumption\",\n",
    "    \"FriedPotato_Consumption\",\n",
    "    \"Age_num\",\n",
    "]\n",
    "\n",
    "BINARY_COLS = [\n",
    "    \"Exercise\",\n",
    "    \"Skin_Cancer\",\n",
    "    \"Other_Cancer\",\n",
    "    \"Depression\",\n",
    "    \"Arthritis\",\n",
    "    \"Sex\",\n",
    "    \"Smoking_History\",\n",
    "]\n",
    "\n",
    "DROP_COLS = [\"Age_Category\"]  # Redundant with Age_num\n",
    "\n",
    "# Validate that required columns exist\n",
    "missing_cols = {\n",
    "    \"categorical\": [col for col in CATEGORICAL_COLS if col not in X_train.columns],\n",
    "    \"numeric\": [col for col in NUMERIC_COLS if col not in X_train.columns],\n",
    "    \"binary\": [col for col in BINARY_COLS if col not in X_train.columns],\n",
    "}\n",
    "\n",
    "if any(missing_cols.values()):\n",
    "    raise ValueError(f\"Missing expected columns in X_train: {missing_cols}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE GROUP DEFINITIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Categorical columns: {len(CATEGORICAL_COLS)}\")\n",
    "print(f\"Numeric columns:     {len(NUMERIC_COLS)}\")\n",
    "print(f\"Binary columns:      {len(BINARY_COLS)}\")\n",
    "print(f\"Columns to drop:     {DROP_COLS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7748f20d-0a79-463c-af67-f5a86b29b053",
   "metadata": {},
   "source": [
    "### Preprocessing: Drop Redundant Columns & Normalize Binary Fields\n",
    "\n",
    "We remove `Age_Category` since we have `Age_num` as its numeric equivalent. This prevents:\n",
    "- Feature redundancy\n",
    "- Multicollinearity issues in Logistic Regression\n",
    "- Wasted memory during GPU training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f34ca15f-0cf0-4412-896a-22c4ac79b633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['Age_Category']\n",
      "Remaining features: 18\n"
     ]
    }
   ],
   "source": [
    "# Drop redundant columns\n",
    "cols_to_drop = [col for col in DROP_COLS if col in X_train.columns]\n",
    "\n",
    "if cols_to_drop:\n",
    "    X_train = X_train.drop(columns=cols_to_drop)\n",
    "    X_test = X_test.drop(columns=cols_to_drop)\n",
    "    print(f\"Dropped columns: {cols_to_drop}\")\n",
    "\n",
    "# Normalize binary fields to 0/1\n",
    "_yes_no_map = {\"Yes\": 1, \"No\": 0}\n",
    "_sex_map = {\"Male\": 1, \"Female\": 0}\n",
    "\n",
    "for col in BINARY_COLS:\n",
    "    if col not in X_train.columns:\n",
    "        continue\n",
    "\n",
    "    if X_train[col].dtype == \"object\":\n",
    "        if set(X_train[col].dropna().unique()).issubset({\"Yes\", \"No\"}):\n",
    "            X_train[col] = X_train[col].map(_yes_no_map)\n",
    "            X_test[col] = X_test[col].map(_yes_no_map)\n",
    "        elif set(X_train[col].dropna().unique()).issubset({\"Male\", \"Female\"}):\n",
    "            X_train[col] = X_train[col].map(_sex_map)\n",
    "            X_test[col] = X_test[col].map(_sex_map)\n",
    "\n",
    "    # Ensure numeric dtype\n",
    "    X_train[col] = pd.to_numeric(X_train[col], errors=\"coerce\")\n",
    "    X_test[col] = pd.to_numeric(X_test[col], errors=\"coerce\")\n",
    "\n",
    "print(f\"Remaining features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b81537-9e3d-4d6d-b574-8e6a14d0e07a",
   "metadata": {},
   "source": [
    "## Step 4: Build Preprocessing Pipeline\n",
    "\n",
    "### Design Philosophy: Encapsulation & Reproducibility\n",
    "\n",
    "We use scikit-learn's `ColumnTransformer` to apply **type-specific preprocessing in a single reproducible object**:\n",
    "\n",
    "1. **Encapsulates Logic** → All preprocessing in one fitted object\n",
    "2. **Prevents Leakage** → Fit only on training data; apply to test\n",
    "3. **Maintains Correspondence** → Feature-to-transformation mapping is explicit\n",
    "4. **Enables Serialization** → Save via `joblib` for production\n",
    "5. **Supports Composition** → Can chain with models in `Pipeline`\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "ColumnTransformer (fit on X_train only)\n",
    "├── Numeric Transformer (8 features)\n",
    "│   ├── SimpleImputer(strategy='median')\n",
    "│   └── StandardScaler()\n",
    "├── Categorical Transformer (3 features)\n",
    "│   ├── SimpleImputer(strategy='most_frequent')\n",
    "│   └── OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "└── Binary Transformer (7 features)\n",
    "    └── SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "Result: Dense NumPy array, GPU-compatible\n",
    "```\n",
    "\n",
    "### Key Parameters\n",
    "\n",
    "| Parameter | Value | Rationale |\n",
    "|-----------|-------|------------|  \n",
    "| `strategy='median'` (numeric) | Robust to outliers | Medical data often has extreme values |\n",
    "| `strategy='most_frequent'` (categorical) | Fast, deterministic | Categorical modes are consistent |\n",
    "| `sparse_output=False` | Dense array | GPU-compatible; no conversion overhead |\n",
    "| `handle_unknown='ignore'` | Unseen categories → zeros | Test set might have rare categories |\n",
    "| `remainder='drop'` | Drop unmapped columns | Removes extraneous columns |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2830ac8-c957-4d7e-a9bd-019e591a3118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREPROCESSING PIPELINE CREATED\n",
      "============================================================\n",
      "Pipeline structure:\n",
      "  Numeric:     Imputer(median) → StandardScaler\n",
      "  Categorical: Imputer(mode) → OneHotEncoder\n",
      "  Binary:      Imputer(mode)\n",
      "\n",
      "Pipeline is NOT yet fitted. Fitting happens in the next step.\n"
     ]
    }
   ],
   "source": [
    "# Numeric feature pipeline\n",
    "numeric_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical feature pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "# Binary feature pipeline\n",
    "binary_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "# Combine into a single ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipeline, NUMERIC_COLS),\n",
    "        (\"cat\", categorical_pipeline, CATEGORICAL_COLS),\n",
    "        (\"bin\", binary_pipeline, BINARY_COLS),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPROCESSING PIPELINE CREATED\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Pipeline structure:\")\n",
    "print(\"  Numeric:     Imputer(median) → StandardScaler\")\n",
    "print(\"  Categorical: Imputer(mode) → OneHotEncoder\")\n",
    "print(\"  Binary:      Imputer(mode)\")\n",
    "print()\n",
    "print(\"Pipeline is NOT yet fitted. Fitting happens in the next step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81f1ba-c947-44d0-940a-13816e40538f",
   "metadata": {},
   "source": [
    "## Step 5: Fit on Training Data & Transform Both Sets\n",
    "\n",
    "### Critical Principle: Data Leakage Prevention\n",
    "\n",
    "**Data leakage** occurs when information from the test set influences the training process, leading to overoptimistic performance estimates.\n",
    "\n",
    "#### How to Prevent Leakage\n",
    "\n",
    "Let $\\theta$ denote learned parameters (μ, σ, categories, modes).\n",
    "\n",
    "**Correct Workflow:**\n",
    "\n",
    "$$\\theta \\leftarrow \\text{fit}(X_{\\text{train}}) \\quad \\text{[Learn from training data only]}$$\n",
    "\n",
    "$$X'_{\\text{train}} = T(X_{\\text{train}}; \\theta)$$\n",
    "\n",
    "$$X'_{\\text{test}} = T(X_{\\text{test}}; \\theta) \\quad \\text{[Transform with SAME parameters]}$$\n",
    "\n",
    "**The Key**: $\\theta$ depends ONLY on $X_{\\text{train}}$, not on $X_{\\text{test}}$.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Scikit-Learn's API enforces this discipline:\n",
    "\n",
    "```python\n",
    "# CORRECT:\n",
    "preprocessor.fit(X_train)                      # Learn params from training only\n",
    "X_train_pre = preprocessor.transform(X_train)  # Apply to training\n",
    "X_test_pre = preprocessor.transform(X_test)    # Apply same params to test\n",
    "\n",
    "# WRONG (causes leakage):\n",
    "preprocessor.fit(X_train_and_test)  # NEVER DO THIS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68d44f5d-902e-421e-8303-f1414eed489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting preprocessor on training data...\n",
      "============================================================\n",
      "PREPROCESSING COMPLETE\n",
      "============================================================\n",
      "X_train_pre shape: (247083, 29)\n",
      "X_test_pre shape:  (61771, 29)\n",
      "Number of features (after OHE): 29\n",
      "Output type: ndarray\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting preprocessor on training data...\")\n",
    "\n",
    "# FIT on training data (learns parameters)\n",
    "# TRANSFORM training data\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# TRANSFORM test data (using parameters learned from training)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "# Extract feature names after transformation\n",
    "feature_names = list(preprocessor.get_feature_names_out())\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"X_train_pre shape: {X_train_pre.shape}\")\n",
    "print(f\"X_test_pre shape:  {X_test_pre.shape}\")\n",
    "print(f\"Number of features (after OHE): {len(feature_names)}\")\n",
    "print(f\"Output type: {type(X_train_pre).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ffbef-63a3-4a52-a9fe-69c16c1bd0b1",
   "metadata": {},
   "source": [
    "## Step 6: Validation & Sanity Checks\n",
    "\n",
    "### Purpose: Silent Corruption Detection\n",
    "\n",
    "Before exporting, we validate that preprocessing has **not introduced silent errors**:\n",
    "\n",
    "| Check | Test | Why It Matters |\n",
    "|-------|------|----------------| \n",
    "| **No Missing Values** | `np.isnan(X_train_pre).sum() == 0` | Imputation succeeded |\n",
    "| **Shape Consistency** | `X_train_pre.shape[1] == X_test_pre.shape[1]` | Train/test alignment |\n",
    "| **Sample Preservation** | `X_train_pre.shape[0] == X_train.shape[0]` | No data loss |\n",
    "| **Stratification Integrity** | `abs(train_pct - test_pct) < 1.0%` | Class ratio preserved |\n",
    "| **Type Consistency** | `isinstance(X_train_pre, np.ndarray)` | GPU-compatible format |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7297bf35-0382-4184-b118-3ae10435bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SANITY CHECKS\n",
      "============================================================\n",
      "\n",
      "NaN values in X_train_pre: 0\n",
      "NaN values in X_test_pre:  0\n",
      "  → All missing values successfully imputed!\n",
      "\n",
      "Feature count matches: True\n",
      "  Train features: 29\n",
      "  Test features:  29\n",
      "\n",
      "Train positive rate: 8.09%\n",
      "Test positive rate:  8.08%\n",
      "  → Stratification preserved: True\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SANITY CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check 1: No NaN values remain\n",
    "nan_train = np.isnan(X_train_pre).sum()\n",
    "nan_test = np.isnan(X_test_pre).sum()\n",
    "print(f\"\\nNaN values in X_train_pre: {nan_train}\")\n",
    "print(f\"NaN values in X_test_pre:  {nan_test}\")\n",
    "\n",
    "if nan_train == 0 and nan_test == 0:\n",
    "    print(\"  → All missing values successfully imputed!\")\n",
    "else:\n",
    "    print(\"WARNING: NaN values detected after preprocessing!\")\n",
    "\n",
    "# Check 2: Shape consistency\n",
    "print(f\"\\nFeature count matches: {X_train_pre.shape[1] == X_test_pre.shape[1]}\")\n",
    "print(f\"  Train features: {X_train_pre.shape[1]}\")\n",
    "print(f\"  Test features:  {X_test_pre.shape[1]}\")\n",
    "\n",
    "# Check 3: Target balance preserved\n",
    "train_positive_rate = y_train.mean() * 100\n",
    "test_positive_rate = y_test.mean() * 100\n",
    "print(f\"\\nTrain positive rate: {train_positive_rate:.2f}%\")\n",
    "print(f\"Test positive rate:  {test_positive_rate:.2f}%\")\n",
    "print(f\"  → Stratification preserved: {abs(train_positive_rate - test_positive_rate) < 1.0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9123c04-4bc6-49ae-b380-79fdf2d69a24",
   "metadata": {},
   "source": [
    "## Step 7: Export Model-Ready Artifacts\n",
    "\n",
    "### Artifact Specification\n",
    "\n",
    "We export **6 core artifacts** to the `processed/` directory:\n",
    "\n",
    "| File | Data Type | Purpose | Primary Consumer |\n",
    "|------|-----------|---------|------------------| \n",
    "| `X_train_ready.joblib` | NumPy array (n_train, n_features) | Training features (preprocessed) | Model training on HPC |\n",
    "| `X_test_ready.joblib` | NumPy array (n_test, n_features) | Test features | Model evaluation |\n",
    "| `y_train_ready.joblib` | 1D array (n_train,) | Training labels (0/1) | Model training targets |\n",
    "| `y_test_ready.joblib` | 1D array (n_test,) | Test labels (0/1) | Model evaluation |\n",
    "| `feature_names.joblib` | List of strings | Feature names after OHE | SHAP/LIME interpretability |\n",
    "| `preprocessor.joblib` | Fitted ColumnTransformer | Serialized preprocessing pipeline | Production inference |\n",
    "\n",
    "### Why `.joblib` Format?\n",
    "\n",
    "- Optimized for scikit-learn objects\n",
    "- Handles NumPy arrays efficiently\n",
    "- Built-in compression reduces storage\n",
    "- Industry standard in ML research\n",
    "\n",
    "### Compression Strategy\n",
    "\n",
    "We use **compression level 3** (default zlib):\n",
    "- Saves ~100–300 MB\n",
    "- Takes <1 sec per file\n",
    "- Balances speed and storage for HPC workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b755406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted intermediate file: X_train_raw.csv\n",
      "Deleted intermediate file: X_test_raw.csv\n",
      "Deleted intermediate file: y_train_raw.csv\n",
      "Deleted intermediate file: y_test_raw.csv\n",
      "✅ Pipeline Cleaned: Only compressed .joblib files remain in /processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define compression level (3 is optimal for speed/size)\n",
    "COMPRESSION = 3\n",
    "\n",
    "# Export model-ready data with compression\n",
    "joblib.dump(X_train_pre, PROCESSED_DIR / 'X_train_ready.joblib', compress=COMPRESSION)\n",
    "joblib.dump(y_train,     PROCESSED_DIR / 'y_train_ready.joblib', compress=COMPRESSION)\n",
    "joblib.dump(X_test_pre,  PROCESSED_DIR / 'X_test_ready.joblib',  compress=COMPRESSION)\n",
    "joblib.dump(y_test,      PROCESSED_DIR / 'y_test_ready.joblib',  compress=COMPRESSION)\n",
    "joblib.dump(feature_names, PROCESSED_DIR / 'feature_names.joblib', compress=COMPRESSION)\n",
    "joblib.dump(preprocessor, PROCESSED_DIR / 'preprocessor.joblib', compress=COMPRESSION)\n",
    "\n",
    "# Cleanup: Delete the raw CSV splits to keep the repo clean\n",
    "raw_files = [x_train_path, x_test_path, y_train_path, y_test_path]\n",
    "for f in raw_files:\n",
    "    if f.exists():\n",
    "        os.remove(f)\n",
    "        print(f\"Deleted intermediate file: {f.name}\")\n",
    "\n",
    "print(\"✅ Pipeline Cleaned: Only compressed .joblib files remain in /processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41482b19-4f4d-45d0-8cb1-6d4d826a624e",
   "metadata": {},
   "source": [
    "## Summary: Preprocessing Pipeline Complete\n",
    "\n",
    "### Workflow Recap\n",
    "\n",
    "| Phase | Input | Process | Output | No-Leakage Guarantee |\n",
    "|-------|-------|---------|--------|---------------------| \n",
    "| **1. Load & Split** | `HeartDisease.csv` | Stratified 80/20 split | X_train, X_test | ✓ Stratify by y |\n",
    "| **2. Analyze Imbalance** | Train/Test labels | Compute positive rate | Class distribution | N/A |\n",
    "| **3. Feature Engineering** | Raw columns | Define feature groups | Feature group lists | N/A |\n",
    "| **4. Build Pipeline** | Feature specs | Compose ColumnTransformer | Unfitted preprocessor | N/A |\n",
    "| **5. Fit & Transform** | X_train, X_test | Fit on train only | X_train_pre, X_test_pre | ✓ Fit on train only |\n",
    "| **6. Validate** | Transformed arrays | Check NaN, shape, stratification | Validation report | ✓ No data loss |\n",
    "| **7. Export** | Preprocessed arrays | Compress with joblib | `.joblib` artifacts | ✓ Frozen artifacts |\n",
    "\n",
    "### Downstream Consumers\n",
    "\n",
    "**Consumer 1: Local Model Development** (`cardiovascular_modelsREAL.ipynb`)\n",
    "- Loads: `X_train_ready.joblib`, `X_test_ready.joblib`, labels\n",
    "- Uses: `feature_names.joblib` for SHAP plots\n",
    "- Trains: Logistic Regression, XGBoost, CatBoost\n",
    "\n",
    "**Consumer 2: HPC Hyperparameter Tuning** (`cardiovascular_optuna_gpu.py`)\n",
    "- Loads: Data → GPU memory on A40 cluster\n",
    "- Optimizer: Optuna with TPE sampler\n",
    "- Objective: Maximize PR-AUC while maintaining ≥85% recall\n",
    "\n",
    "**Consumer 3: Explainability** (`cardio_SHAP.ipynb`)\n",
    "- Loads: `feature_names.joblib`, best model\n",
    "- Methods: SHAP TreeExplainer or KernelExplainer\n",
    "- Outputs: Feature importance plots, clinical insights\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "| Principle | Implementation | Benefit |\n",
    "|-----------|----------------|---------|  \n",
    "| **Stratified Splitting** | `stratify=y` | Preserves disease prevalence |\n",
    "| **Fit on Train Only** | `preprocessor.fit(X_train)` | Prevents data leakage |\n",
    "| **Type-Specific Pipelines** | Separate transformers | Appropriate handling |\n",
    "| **Median Imputation** | Robust to outliers | Better for skewed distributions |\n",
    "| **StandardScaler** | Zero mean, unit variance | Convergence, GPU stability |\n",
    "| **One-Hot Encoding** | Dense format | GPU-compatible |\n",
    "| **Class Weighting** | `class_weight='balanced'` | Handles imbalance without synthetic data |\n",
    "| **Comprehensive Validation** | NaN/shape checks | Detects silent corruption |\n",
    "| **Deterministic Export** | Fixed `RANDOM_STATE` | Reproducible artifacts |\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This preprocessing notebook **successfully transforms raw clinical data into production-ready artifacts**. The pipeline:\n",
    "\n",
    "* **Prevents data leakage** via training-only fitting\n",
    "* **Preserves clinical validity** through stratified splitting\n",
    "* **Handles missing data** defensively\n",
    "* **Optimizes for GPU** with dense arrays\n",
    "* **Enables reproducibility** via fixed seeds\n",
    "* **Documents reasoning** for interpretability\n",
    "\n",
    "**Next Steps:**\n",
    "1. Load artifacts in `cardiovascular_modelsREAL.ipynb` for local experiments\n",
    "2. Deploy to HPC cluster and run `cardiovascular_optuna_gpu.py` for GPU tuning\n",
    "3. Use best model in `cardio_SHAP.ipynb` for explainability analysis\n",
    "4. Communicate findings to clinical stakeholders with SHAP plots"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
